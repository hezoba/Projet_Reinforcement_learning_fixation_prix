# -*- coding: utf-8 -*-
"""Untitled32.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WXXSByZK9Y2vHRQ-AmD6hEkg0ijLNYyn
"""

pip  install gradio

import gradio as gr
import numpy as np
import pickle
from google.colab import drive

# Monter Google Drive
drive.mount('/content/drive')

# Charger la table Q sauvegardée
q_table_path = '/content/drive/My Drive/Projet_RL/q_table.npy'

# Charger la table Q
with open(q_table_path, 'rb') as f:
    q_table = np.load(f, allow_pickle=True).item()

# Fonction pour déterminer l'action optimale pour un état donné
def determine_optimal_action(prix_entreprise, prix_concurrents, demande_marche,
                              stock_marche_moyen, cout_entreprise, stock_entreprise, qte_vendue):
    current_state = {
        'prix_entreprise': prix_entreprise,
        'prix_concurrents': prix_concurrents,
        'demande_marche': demande_marche,
        'stock_marche_moyen': stock_marche_moyen,
        'cout_entreprise': cout_entreprise,
        'stock_entreprise': stock_entreprise,
        'qte_vendue': qte_vendue
    }

    # Convertir l'état actuel en une clé hashable
    current_state_key = tuple(current_state.items())

    # Déterminer l'action optimale
    if current_state_key in q_table:
        best_action = max(q_table[current_state_key], key=q_table[current_state_key].get)
    else:
        # Si l'état n'est pas dans la table Q, initialiser les valeurs Q pour chaque action
        q_table[current_state_key] = {
            'Diminuer le prix': 0.0,
            'Augmenter le prix': 0.0,
            'Maintenir le prix': 0.0
        }

        # Choisir une action aléatoire pour cet état nouvellement ajouté
        best_action = np.random.choice(['Diminuer le prix', 'Augmenter le prix', 'Maintenir le prix'])

        # Définir une récompense minimal pour entraîner l'état

        reward = 100

        # Mettre à jour la table Q avec la récompense fictive

        best_next_action = best_action
        td_target = reward  # Pas de next state, donc on utilise la récompense
        td_delta = td_target - q_table[current_state_key][best_next_action]
        q_table[current_state_key][best_next_action] += 0.1 * td_delta  # Mettez à jour la valeur Q

    return best_action

# Fonction de l'interface Gradio
def gradio_interface(prix_entreprise, prix_concurrents, demande_marche, stock_marche_moyen,
                     cout_entreprise, stock_entreprise, qte_vendue):
    action_optimale = determine_optimal_action(prix_entreprise, prix_concurrents, demande_marche,
                                               stock_marche_moyen, cout_entreprise, stock_entreprise, qte_vendue)
    return action_optimale

# Définir les entrées et la sortie de l'interface Gradio
inputs = [
    gr.Number(label="Prix de l'entreprise"),
    gr.Number(label="Prix des concurrents"),
    gr.Number(label="Demande du marché"),
    gr.Number(label="Stock moyen du marché"),
    gr.Number(label="Coût de l'entreprise"),
    gr.Number(label="Stock de l'entreprise"),
    gr.Number(label="Quantité vendue")
]

outputs = gr.Textbox(label="Action optimale")

# Créer l’application Gradio
gr.Interface(fn=gradio_interface, inputs=inputs, outputs=outputs,
             title="Prédiction de l'action optimale",
             description="Entrez les paramètres de marché pour obtenir l'action optimale à prendre selon le modèle Q-Learning.").launch()